# external_validate_A_Data1.py
import os
import json
from datetime import datetime
import copy
import warnings

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.impute import KNNImputer
from sklearn.metrics import (
    accuracy_score, f1_score, roc_auc_score,
    precision_score, recall_score,
    confusion_matrix
)
from joblib import load

warnings.filterwarnings("ignore")

# -------- Pretty table deps (optional) --------
try:
    from rich.console import Console
    from rich.table import Table

    _HAS_RICH = True
except Exception:
    _HAS_RICH = False

try:
    from tabulate import tabulate

    _HAS_TABULATE = True
except Exception:
    _HAS_TABULATE = False


# ===========================
# Â∞èÂ∑•ÂÖ∑ÔºöSpecificity / NPV
# ===========================
def specificity_npv(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])
    if cm.shape != (2, 2):
        return np.nan, np.nan
    tn, fp, fn, tp = cm.ravel()
    spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan
    npv = tn / (tn + fn) if (tn + fn) > 0 else np.nan
    return spec, npv


# ===========================
# Pretty table printer
# ===========================
def pretty_print_table(df, title=None, float_cols=None, float_digits=4):
    if float_cols is None:
        float_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    df_show = df.copy()
    for c in float_cols:
        df_show[c] = df_show[c].astype(float).round(float_digits)

    if _HAS_RICH:
        console = Console()
        if title:
            console.rule(f"[bold]{title}")
        table = Table(show_header=True, header_style="bold", box=None, pad_edge=False)
        for c in df_show.columns:
            align = "right" if c in float_cols else "left"
            table.add_column(str(c), justify=align, no_wrap=True)
        for _, row in df_show.iterrows():
            row_vals = []
            for c in df_show.columns:
                v = row[c]
                if pd.isna(v):
                    row_vals.append("-")
                elif c in float_cols:
                    row_vals.append(f"{float(v):.{float_digits}f}")
                else:
                    row_vals.append(str(v))
            table.add_row(*row_vals)
        console.print(table)
        return

    if _HAS_TABULATE:
        print(
            tabulate(
                df_show,
                headers="keys",
                tablefmt="github",
                showindex=False,
                floatfmt=f".{float_digits}f",
            )
        )
        return

    # fallback: Á¥îÊñáÂ≠óÂ∞çÈΩä
    col_widths = {}
    for c in df_show.columns:
        max_val_len = max(
            [
                len(f"{v:.{float_digits}f}")
                if (c in float_cols and pd.notna(v))
                else len(str(v))
                for v in df_show[c]
            ]
            + [len(str(c))]
        )
        col_widths[c] = max_val_len
    if title:
        print("\n" + title)
    header = "  ".join(str(c).ljust(col_widths[c]) for c in df_show.columns)
    print(header)
    print("-" * len(header))
    for _, row in df_show.iterrows():
        parts = []
        for c in df_show.columns:
            v = row[c]
            if pd.isna(v):
                s = "-"
            elif c in float_cols:
                s = f"{float(v):.{float_digits}f}"
            else:
                s = str(v)
            align = str.ljust if c not in float_cols else str.rjust
            parts.append(align(s, col_widths[c]))
        print("  ".join(parts))


# ===========================
# Â§ñÈÉ®È©óË≠âÂ∞àÁî® DataProcessor
# ÔºàÈÇèËºØË¶ÅË∑ü A ÁµÑ training ‰∏ÄËá¥Ôºâ
# ===========================
class ExternalDataProcessorBaseline:
    """
    - Ë∑ü A ÁµÑ‰∏ÄÊ®£ÔºöHRV: SDNN, LF, HF, LFHF + Age, Sex, BMI
    - Â∑•Á®ãÁâπÂæµÔºöHRV_Mean, LF_HF_Ratio
    - Áî®„ÄåË®ìÁ∑¥ÊôÇÂ≠ò‰∏ã‰æÜÁöÑ outlier_bounds / scaler / imputer„ÄçÂÅöËΩâÊèõ
    """

    def __init__(self, treat_zero_as_missing_in_hrv=True):
        self.hrv_features = ["SDNN", "LF", "HF", "LFHF"]
        self.basic_features = ["Age", "Sex", "BMI"]
        self.log_hrv_cols = ["LF", "HF", "LFHF"]
        self.log_engineered_cols = ["HRV_Mean", "LF_HF_Ratio"]
        self.treat_zero_as_missing_in_hrv = treat_zero_as_missing_in_hrv

    def build_raw_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Âæû Data1 ÁöÑÂéüÂßãÊ¨Ñ‰ΩçÂª∫Á´ã HRV + basic + Â∑•Á®ãÁâπÂæµ"""
        all_features = self.basic_features + self.hrv_features
        available = [f for f in all_features if f in df.columns]
        X = df[available].copy()

        hrv_cols = [c for c in self.hrv_features if c in X.columns]
        if len(hrv_cols) >= 3:
            X["HRV_Mean"] = X[hrv_cols].mean(axis=1)
        if "LF" in X.columns and "HF" in X.columns:
            X["LF_HF_Ratio"] = X["LF"] / (X["HF"] + 1e-6)
            X["LF_HF_Ratio"].replace([np.inf, -np.inf], np.nan, inplace=True)

        return X

    def _numeric_feature_list_for_outlier(self, X_frame: pd.DataFrame):
        candidates = []
        for col in (self.hrv_features + ["Age", "BMI"]):
            if col in X_frame.columns:
                candidates.append(col)
        for col in ["HRV_Mean", "LF_HF_Ratio"]:
            if col in X_frame.columns:
                candidates.append(col)
        out = []
        for c in candidates:
            s = pd.to_numeric(X_frame[c], errors="coerce")
            if s.notnull().any():
                out.append(c)
        return out

    def _apply_outlier_to_nan(self, X_frame: pd.DataFrame, outlier_bounds: dict):
        if not outlier_bounds:
            return X_frame

        Xp = X_frame.copy()

        # zero ‚Üí NaN for HRV
        if self.treat_zero_as_missing_in_hrv:
            for col in [c for c in self.hrv_features if c in Xp.columns]:
                s = pd.to_numeric(Xp[col], errors="coerce")
                zero_mask = (s == 0)
                Xp.loc[zero_mask, col] = np.nan

        for col, (lb, ub) in outlier_bounds.items():
            if col not in Xp.columns:
                continue
            s = pd.to_numeric(Xp[col], errors="coerce")
            mask = (s < lb) | (s > ub)
            Xp.loc[mask, col] = np.nan

        return Xp

    def _apply_log1p(self, X_frame: pd.DataFrame):
        Xp = X_frame.copy()
        for col in self.log_hrv_cols + self.log_engineered_cols:
            if col not in Xp.columns:
                continue
            s = pd.to_numeric(Xp[col], errors="coerce")
            neg_mask = s < 0
            if neg_mask.any():
                Xp.loc[neg_mask, col] = np.nan
            Xp[col] = np.log1p(Xp[col])
        return Xp

    def apply_full_transform(
        self,
        X_raw: pd.DataFrame,
        feature_columns: list,
        outlier_bounds: dict,
        imputer: KNNImputer,
        scaler: StandardScaler,
    ) -> pd.DataFrame:
        """
        ‰ΩøÁî®Ë®ìÁ∑¥ÊôÇÁöÑ outlier_bounds / imputer / scaler
        Â∞ç Data1 ÂÅöÂÆåÊï¥ÂâçËôïÁêÜÔºåËº∏Âá∫Ê¨Ñ‰ΩçÈ†ÜÂ∫è = feature_columns
        """
        Xp = X_raw.copy()

        # 1) Ë£ú‰∏äÁº∫ÁöÑÊ¨Ñ‰ΩçÔºåÁ¢∫‰øùË∑üË®ìÁ∑¥ÊôÇ feature_columns ‰∏ÄËá¥
        for col in feature_columns:
            if col not in Xp.columns:
                Xp[col] = np.nan
        Xp = Xp[feature_columns].copy()

        # 2) Èõ¢Áæ§ÂÄº ‚Üí NaN
        Xp = self._apply_outlier_to_nan(Xp, outlier_bounds)

        # 3) log1p
        Xp = self._apply_log1p(Xp)

        # 4) KNNImputer
        knn_f = self._numeric_feature_list_for_outlier(Xp)
        if imputer is not None and len(knn_f) > 0:
            try:
                Xp[knn_f] = imputer.transform(Xp[knn_f])
            except Exception as e:
                print(f"‚ö†Ô∏è KNNImputer transform Â§±ÊïóÔºåÊîπÁî®‰∏≠‰ΩçÊï∏Ë£úÂÄº: {e}")

        # 5) ‰ªçÊúâ NaN Â∞±Áî® Data1 ÁöÑ‰∏≠‰ΩçÊï∏Ë£ú
        if Xp.isnull().any().any():
            Xp.fillna(Xp.median(numeric_only=True), inplace=True)

        # 6) StandardScalerÔºàË∑ü training ‰∏ÄÊ®£Ôºå‰∏çËôïÁêÜ SexÔºâ
        cols = Xp.columns.tolist()
        num_cols = [c for c in cols if c != "Sex" and pd.api.types.is_numeric_dtype(Xp[c])]
        other_cols = [c for c in cols if c not in num_cols]

        if scaler is not None and len(num_cols) > 0:
            X_num = pd.DataFrame(
                scaler.transform(Xp[num_cols]),
                columns=num_cols,
                index=Xp.index,
            )
        else:
            X_num = Xp[num_cols].copy()

        X_scaled = pd.concat([X_num, Xp[other_cols]], axis=1)[cols]
        return X_scaled


# ===========================
# Â§ñÈÉ®È©óË≠â‰∏ªÊµÅÁ®ã
# ===========================
def main():
    print("\n" + "=" * 70)
    print("üèÅ Â§ñÈÉ®È©óË≠âÔºö‰ΩøÁî® A ÁµÑÊúÄ‰Ω≥Ê®°Âûã ‚Üí Data1 (External Test)")
    print("=" * 70)

    TRAIN_RUN_DIR = r"D:\FLY114-main\HRV-Project\data2_baseline"  # ‚Üê ‰Ω† A ÁµÑË®ìÁ∑¥Â•ΩÁöÑË≥áÊñôÂ§æ
    FILE_PATH = r"D:\FLY114-main\data.xlsx"
    SHEET_NAME_TEST = "Data1"   # Data1 ÁöÑÂ∑•‰ΩúË°®ÂêçÁ®±
    # =====================================

    models_dir = os.path.join(TRAIN_RUN_DIR, "models")
    if not os.path.isdir(models_dir):
        print(f"‚ùå Êâæ‰∏çÂà∞ models ÁõÆÈåÑÔºö{models_dir}")
        return

    # Âª∫‰∏ÄÂÄãÊñ∞ÁöÑËº∏Âá∫Ë≥áÊñôÂ§æÂ≠òÂ§ñÈÉ®È©óË≠âÁµêÊûú
    timestamp = datetime.now().strftime("External_Data1_%Y%m%d_%H%M%S")
    out_dir = os.path.join(TRAIN_RUN_DIR, timestamp)
    os.makedirs(out_dir, exist_ok=True)
    print(f"üìÇ Â§ñÈÉ®È©óË≠âËº∏Âá∫Ë≥áÊñôÂ§æ: {out_dir}")

    # ËÆÄ Data1
    try:
        df_test = pd.read_excel(FILE_PATH, sheet_name=SHEET_NAME_TEST)
        print(f"‚úì Data1 ËºâÂÖ•ÊàêÂäü: {df_test.shape[0]} Á≠ÜÔºàÂ∑•‰ΩúË°®Ôºö{SHEET_NAME_TEST}Ôºâ")
    except Exception as e:
        print(f"‚ùå ÁÑ°Ê≥ïËÆÄÂèñ Data1Ôºö{e}")
        return

    label_names = ["Health", "SSD", "MDD", "Panic", "GAD"]
    processor = ExternalDataProcessorBaseline(treat_zero_as_missing_in_hrv=True)

    metrics_rows = []
    pred_sheets = {}

    for label in label_names:
        print("\n" + "-" * 60)
        print(f"üîç ËôïÁêÜÊ®ôÁ±§Ôºö{label}")

        meta_path = os.path.join(models_dir, f"{label}_best.json")
        if not os.path.isfile(meta_path):
            print(f"‚ö†Ô∏è Êâæ‰∏çÂà∞ meta Ê™îÔºö{meta_path}ÔºåÁï•ÈÅéÊ≠§ label")
            continue

        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)

        feature_columns = meta.get("feature_columns", [])
        outlier_bounds = meta.get("outlier_bounds", {})
        threshold = float(meta.get("threshold", 0.5))
        files = meta.get("files", {})

        model_file = files.get("model")
        scaler_file = files.get("scaler")
        imputer_file = files.get("imputer")

        if not model_file:
            print(f"‚ö†Ô∏è {label}: meta ‰∏≠Ê≤íÊúâ model Ê™îÂêçÔºåÁï•ÈÅé")
            continue

        model_path = os.path.join(models_dir, model_file)
        if not os.path.isfile(model_path):
            print(f"‚ö†Ô∏è {label}: Êâæ‰∏çÂà∞Ê®°ÂûãÊ™îÔºö{model_path}ÔºåÁï•ÈÅé")
            continue

        model = load(model_path)
        scaler = load(os.path.join(models_dir, scaler_file)) if scaler_file else None
        imputer = load(os.path.join(models_dir, imputer_file)) if imputer_file else None

        # ===== Ê∫ñÂÇô X_test_rawÔºàData1Ôºâ =====
        X_raw = processor.build_raw_features(df_test)
        X_test = processor.apply_full_transform(
            X_raw=X_raw,
            feature_columns=feature_columns,
            outlier_bounds=outlier_bounds,
            imputer=imputer,
            scaler=scaler,
        )

        # Âèñ Data1 ÁöÑÁúüÂØ¶Ê®ôÁ±§ÔºàÂ¶ÇÊûúÊúâÔºâ
        if label in df_test.columns:
            y_true = df_test[label].astype(int).values
        else:
            y_true = None

        # ===== Êé®Ë´ñ =====
        try:
            proba = model.predict_proba(X_test)[:, 1]
        except Exception:
            # Êúâ‰∫õÊ®°ÂûãÔºà‰æãÂ¶ÇÊüê‰∫õ TreeÔºâÂèØËÉΩÊ≤íÊúâ predict_proba
            try:
                decision = model.decision_function(X_test)
                proba = (decision - decision.min()) / (decision.max() - decision.min() + 1e-8)
            except Exception as e:
                print(f"‚ùå {label}: Ê®°ÂûãÁÑ°Ê≥ïËº∏Âá∫Ê©üÁéáÔºö{e}")
                continue

        y_pred = (proba >= threshold).astype(int)

        # ===== Ë©ï‰º∞ÊåáÊ®ô =====
        if y_true is not None and len(np.unique(y_true)) > 1:
            f1 = f1_score(y_true, y_pred)
            acc = accuracy_score(y_true, y_pred)
            try:
                auc = roc_auc_score(y_true, proba)
            except Exception:
                auc = np.nan
            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)
            spec, npv = specificity_npv(y_true, y_pred)
        else:
            # Ê≤íÊúâÁúüÂØ¶Ê®ôÁ±§Â∞±Âè™Â≠òÈ†êÊ∏¨ÁµêÊûú
            f1 = acc = auc = precision = recall = spec = npv = np.nan

        metrics_rows.append(
            {
                "Label": label,
                "Threshold_used": threshold,
                "F1": f1,
                "Precision": precision,
                "Recall": recall,
                "Spec": spec,
                "NPV": npv,
                "AUC": auc,
                "ACC": acc,
            }
        )

        # Â≠òÊØèÂÄãÁóÖ‰∫∫ÁöÑÈ†êÊ∏¨
        pred_df = pd.DataFrame(
            {
                "Index": df_test.index,
                f"{label}_y_true": y_true if y_true is not None else np.nan,
                f"{label}_y_pred": y_pred,
                f"{label}_proba": proba,
            }
        )
        pred_sheets[label] = pred_df

        print(
            f"   ‚Üí {label:<6} | "
            f"F1={f1:.4f}, P={precision:.4f}, R={recall:.4f}, "
            f"Spec={spec:.4f}, NPV={npv:.4f}, AUC={auc:.4f}, ACC={acc:.4f}, t={threshold:.2f}"
        )

    # ====== ÊåáÊ®ôÂΩôÊï¥Ëº∏Âá∫ ======
    if metrics_rows:
        metrics_df = pd.DataFrame(
            metrics_rows,
            columns=[
                "Label",
                "Threshold_used",
                "F1",
                "Precision",
                "Recall",
                "Spec",
                "NPV",
                "AUC",
                "ACC",
            ],
        )
        metrics_path = os.path.join(out_dir, "External_Data1_Metrics.xlsx")
        metrics_df.to_excel(metrics_path, index=False)
        pretty_print_table(
            metrics_df,
            title="External Validation on Data1 (A ÁµÑÊúÄ‰Ω≥Ê®°Âûã)",
            float_cols=["F1", "Precision", "Recall", "Spec", "NPV", "AUC", "ACC"],
            float_digits=4,
        )
        print(f"\n‚úÖ Â§ñÈÉ®È©óË≠âÊåáÊ®ôÂ∑≤Ëº∏Âá∫Ôºö{metrics_path}")
    else:
        print("‚ö†Ô∏è Ê≤íÊúâ‰ªª‰Ωï label ÊàêÂäüË®àÁÆóÊåáÊ®ô„ÄÇ")

    # ====== ÂêÑ label ÁöÑÂÄãÊ°àÈ†êÊ∏¨Ëº∏Âá∫ ======
    if pred_sheets:
        preds_path = os.path.join(out_dir, "External_Data1_Predictions.xlsx")
        with pd.ExcelWriter(preds_path) as writer:
            for label, df_pred in pred_sheets.items():
                df_pred.to_excel(writer, sheet_name=label, index=False)
        print(f"‚úÖ ÂÄãÊ°àÂ±§Á¥öÈ†êÊ∏¨Â∑≤Ëº∏Âá∫Ôºö{preds_path}")

    print("\nüéâ Â§ñÈÉ®È©óË≠âÂÆåÊàêÔºÅ")
    print("=" * 70)


if __name__ == "__main__":
    main()